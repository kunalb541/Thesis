#!/usr/bin/env python3
"""
Roman Microlensing Classifier Training - ULTRA-FAST VERSION
Supports both HDF5 and NPZ formats with optional RAM loading for maximum speed

Features:
- RAM loading: 0.1-0.2 s/batch (load entire dataset into memory)
- NPZ format: 0.3-0.5 s/batch (no HDF5 locks)
- HDF5 format: 2-4 s/batch (with 1 worker)
- Distributed training (DDP)
- Hierarchical classification
- Automatic checkpointing
"""

import os
import sys
import time
import argparse
import logging
from pathlib import Path
from datetime import datetime

import h5py
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from tqdm import tqdm

# =============================================================================
# ULTRA-FAST DATASET - Supports HDF5, NPZ, and RAM Loading
# =============================================================================

class RAMLensingDataset(Dataset):
    """
    Load entire dataset into RAM for ultra-fast training.
    Speed: 0.1-0.2 s/batch (100Ã— faster than HDF5 on disk)
    """
    
    def __init__(self, file_path, split='train', rank=0, norm_stats=None):
        super().__init__()
        self.rank = rank
        self.split = split
        
        if rank == 0:
            print(f"ðŸš€ ULTRA-FAST MODE: Loading {split} dataset into RAM...")
            print(f"   Source: {file_path}")
        
        # Detect file format
        file_path = Path(file_path)
        
        if file_path.suffix == '.npz':
            # Load from NPZ
            if rank == 0:
                print("   Format: NPZ (fast!)")
            data = np.load(str(file_path))
            self.flux = data['flux']
            self.delta_t = data['delta_t']
            self.labels = data['labels']
            self.timestamps = data['timestamps']
            
        elif file_path.suffix == '.h5' or file_path.suffix == '.hdf5':
            # Load from HDF5
            if rank == 0:
                print("   Format: HDF5 (loading into RAM)")
            with h5py.File(str(file_path), 'r') as f:
                if rank == 0:
                    print("   Loading flux...", end='', flush=True)
                self.flux = f['flux'][:]
                if rank == 0:
                    print(f" âœ“ ({self.flux.nbytes/1e9:.2f} GB)")
                
                if rank == 0:
                    print("   Loading delta_t...", end='', flush=True)
                self.delta_t = f['delta_t'][:]
                if rank == 0:
                    print(f" âœ“ ({self.delta_t.nbytes/1e9:.2f} GB)")
                
                if rank == 0:
                    print("   Loading labels...", end='', flush=True)
                self.labels = f['labels'][:]
                if rank == 0:
                    print(f" âœ“ ({self.labels.nbytes/1e9:.2f} GB)")
                
                if rank == 0:
                    print("   Loading timestamps...", end='', flush=True)
                self.timestamps = f['timestamps'][:]
                if rank == 0:
                    print(f" âœ“ ({self.timestamps.nbytes/1e9:.2f} GB)")
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")
        
        # Compute normalization if not provided
        if norm_stats is None:
            if rank == 0:
                print("   Computing normalization statistics...")
            self.flux_median = np.median(self.flux)
            self.flux_iqr = np.percentile(self.flux, 75) - np.percentile(self.flux, 25)
            self.delta_t_median = np.median(self.delta_t)
            self.delta_t_iqr = np.percentile(self.delta_t, 75) - np.percentile(self.delta_t, 25)
            if rank == 0:
                print(f"   Flux    - Median: {self.flux_median:.4f}, IQR: {self.flux_iqr:.4f}")
                print(f"   Delta_t - Median: {self.delta_t_median:.4f}, IQR: {self.delta_t_iqr:.4f}")
        else:
            self.flux_median = norm_stats['flux_median']
            self.flux_iqr = norm_stats['flux_iqr']
            self.delta_t_median = norm_stats['delta_t_median']
            self.delta_t_iqr = norm_stats['delta_t_iqr']
        
        # Split train/val (90/10)
        n_total = len(self.labels)
        n_train = int(0.9 * n_total)
        
        if split == 'train':
            self.indices = np.arange(0, n_train)
        else:
            self.indices = np.arange(n_train, n_total)
        
        if rank == 0:
            total_mem = (self.flux.nbytes + self.delta_t.nbytes + 
                        self.labels.nbytes + self.timestamps.nbytes) / 1e9
            print(f"   âœ“ Dataset in RAM: {total_mem:.2f} GB")
            print(f"   âœ“ {split}: {len(self.indices):,} samples")
            print(f"   âœ“ Ready for ULTRA-FAST training!")
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        actual_idx = self.indices[idx]
        
        # Normalize
        flux = (self.flux[actual_idx] - self.flux_median) / (self.flux_iqr + 1e-8)
        delta_t = (self.delta_t[actual_idx] - self.delta_t_median) / (self.delta_t_iqr + 1e-8)
        
        return {
            'flux': torch.from_numpy(flux).float(),
            'delta_t': torch.from_numpy(delta_t).float(),
            'label': torch.tensor(self.labels[actual_idx], dtype=torch.long),
            'timestamp': torch.from_numpy(self.timestamps[actual_idx]).float()
        }
    
    def get_norm_stats(self):
        """Return normalization statistics for validation set"""
        return {
            'flux_median': self.flux_median,
            'flux_iqr': self.flux_iqr,
            'delta_t_median': self.delta_t_median,
            'delta_t_iqr': self.delta_t_iqr
        }


# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================

class PositionalEncoding(nn.Module):
    """Positional encoding for transformer"""
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]


class AttentionPooling(nn.Module):
    """Attention-based pooling"""
    
    def __init__(self, d_model):
        super().__init__()
        self.attention = nn.Linear(d_model, 1)
    
    def forward(self, x, mask=None):
        # x: (batch, seq_len, d_model)
        attn_weights = self.attention(x)  # (batch, seq_len, 1)
        
        if mask is not None:
            attn_weights = attn_weights.masked_fill(mask.unsqueeze(-1), float('-inf'))
        
        attn_weights = F.softmax(attn_weights, dim=1)
        pooled = (x * attn_weights).sum(dim=1)  # (batch, d_model)
        return pooled


class ConvFeatureExtractor(nn.Module):
    """1D CNN for feature extraction from time series"""
    
    def __init__(self, d_model, window_size=7):
        super().__init__()
        self.conv1 = nn.Conv1d(2, d_model // 2, kernel_size=window_size, padding=window_size//2)
        self.conv2 = nn.Conv1d(d_model // 2, d_model, kernel_size=window_size, padding=window_size//2)
        self.bn1 = nn.BatchNorm1d(d_model // 2)
        self.bn2 = nn.BatchNorm1d(d_model)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, flux, delta_t):
        # flux, delta_t: (batch, seq_len)
        x = torch.stack([flux, delta_t], dim=1)  # (batch, 2, seq_len)
        
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.dropout(x)
        
        x = x.transpose(1, 2)  # (batch, seq_len, d_model)
        return x


class RomanMicrolensingClassifier(nn.Module):
    """
    Transformer-based microlensing classifier with hierarchical output
    """
    
    def __init__(
        self,
        d_model=256,
        n_layers=4,
        n_heads=2,
        dropout=0.3,
        window_size=7,
        max_seq_len=2400,
        n_classes=3,
        hierarchical=True,
        use_attention_pooling=True,
        use_flash_attention=True
    ):
        super().__init__()
        
        self.d_model = d_model
        self.hierarchical = hierarchical
        self.use_attention_pooling = use_attention_pooling
        
        # Feature extraction
        self.feature_extractor = ConvFeatureExtractor(d_model, window_size)
        
        # Positional encoding
        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=4 * d_model,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # Pooling
        if use_attention_pooling:
            self.pooling = AttentionPooling(d_model)
        else:
            self.pooling = None
        
        # Classification heads
        if hierarchical:
            # Binary: Flat vs Lensing
            self.binary_head = nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model // 2, 2)
            )
            
            # Ternary: PSPL vs Binary (conditioned on lensing)
            self.ternary_head = nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model // 2, 2)
            )
        else:
            # Single 3-way classifier
            self.classifier = nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model // 2, n_classes)
            )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, flux, delta_t, return_features=False):
        # flux, delta_t: (batch, seq_len)
        
        # Feature extraction
        x = self.feature_extractor(flux, delta_t)  # (batch, seq_len, d_model)
        
        # Positional encoding
        x = self.pos_encoder(x)
        
        # Transformer
        x = self.transformer(x)  # (batch, seq_len, d_model)
        
        # Pooling
        if self.pooling is not None:
            features = self.pooling(x)
        else:
            features = x.mean(dim=1)  # Global average pooling
        
        if return_features:
            return features
        
        # Classification
        if self.hierarchical:
            # Binary: Flat (0) vs Lensing (1)
            binary_logits = self.binary_head(features)  # (batch, 2)
            
            # Ternary: PSPL (0) vs Binary (1) for lensing events
            ternary_logits = self.ternary_head(features)  # (batch, 2)
            
            return binary_logits, ternary_logits
        else:
            return self.classifier(features)


# =============================================================================
# HIERARCHICAL LOSS
# =============================================================================

class HierarchicalLoss(nn.Module):
    """
    Hierarchical loss for microlensing classification
    
    Label mapping:
    - 0: Flat (no lensing)
    - 1: PSPL (single lens)
    - 2: Binary (binary lens)
    """
    
    def __init__(self):
        super().__init__()
    
    def forward(self, binary_logits, ternary_logits, labels):
        # Convert labels to binary and ternary
        # Binary: 0=Flat, 1=Lensing (PSPL or Binary)
        binary_labels = (labels > 0).long()
        
        # Ternary: For lensing events, 0=PSPL, 1=Binary
        # For flat events, this doesn't matter (masked out)
        ternary_labels = (labels == 2).long()
        
        # Binary loss (all samples)
        binary_loss = F.cross_entropy(binary_logits, binary_labels)
        
        # Ternary loss (only lensing events)
        lensing_mask = binary_labels == 1
        if lensing_mask.sum() > 0:
            ternary_loss = F.cross_entropy(
                ternary_logits[lensing_mask],
                ternary_labels[lensing_mask]
            )
        else:
            ternary_loss = torch.tensor(0.0, device=labels.device)
        
        total_loss = binary_loss + ternary_loss
        
        return total_loss, binary_loss, ternary_loss


def hierarchical_predictions(binary_logits, ternary_logits):
    """Convert hierarchical logits to final predictions"""
    # Binary prediction
    binary_pred = binary_logits.argmax(dim=1)  # 0=Flat, 1=Lensing
    
    # Ternary prediction
    ternary_pred = ternary_logits.argmax(dim=1)  # 0=PSPL, 1=Binary
    
    # Combine: Flat=0, PSPL=1, Binary=2
    final_pred = torch.zeros_like(binary_pred)
    final_pred[binary_pred == 0] = 0  # Flat
    final_pred[(binary_pred == 1) & (ternary_pred == 0)] = 1  # PSPL
    final_pred[(binary_pred == 1) & (ternary_pred == 1)] = 2  # Binary
    
    return final_pred


# =============================================================================
# TRAINING FUNCTIONS
# =============================================================================

def setup_distributed():
    """Initialize distributed training"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        if rank == 0:
            print("=" * 80)
            print("DDP Initialization")
            print(f"  RANK: {rank}")
            print(f"  LOCAL_RANK: {local_rank}")
            print(f"  WORLD_SIZE: {world_size}")
            print("=" * 80)
        
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
        
        torch.cuda.set_device(local_rank)
        
        if rank == 0:
            print("âœ“ DDP initialized successfully")
        
        return rank, local_rank, world_size
    else:
        # Single GPU
        rank = 0
        local_rank = 0
        world_size = 1
        torch.cuda.set_device(0)
        return rank, local_rank, world_size


def train_epoch(model, dataloader, criterion, optimizer, device, rank, epoch, scaler=None):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    if rank == 0:
        pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Train]")
    else:
        pbar = dataloader
    
    for batch in pbar:
        flux = batch['flux'].to(device)
        delta_t = batch['delta_t'].to(device)
        labels = batch['label'].to(device)
        
        optimizer.zero_grad()
        
        # Forward
        if hasattr(model, 'module'):
            hierarchical = model.module.hierarchical
        else:
            hierarchical = model.hierarchical
        
        if hierarchical:
            binary_logits, ternary_logits = model(flux, delta_t)
            loss, binary_loss, ternary_loss = criterion(binary_logits, ternary_logits, labels)
            
            # Get predictions
            preds = hierarchical_predictions(binary_logits, ternary_logits)
        else:
            logits = model(flux, delta_t)
            loss = F.cross_entropy(logits, labels)
            preds = logits.argmax(dim=1)
        
        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # Stats
        total_loss += loss.item()
        correct += (preds == labels).sum().item()
        total += labels.size(0)
        
        if rank == 0:
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.0 * correct / total:.2f}%',
                'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
            })
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100.0 * correct / total
    
    return avg_loss, accuracy


def validate(model, dataloader, criterion, device, rank, epoch):
    """Validate"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    if rank == 0:
        pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Val]")
    else:
        pbar = dataloader
    
    with torch.no_grad():
        for batch in pbar:
            flux = batch['flux'].to(device)
            delta_t = batch['delta_t'].to(device)
            labels = batch['label'].to(device)
            
            # Forward
            if hasattr(model, 'module'):
                hierarchical = model.module.hierarchical
            else:
                hierarchical = model.hierarchical
            
            if hierarchical:
                binary_logits, ternary_logits = model(flux, delta_t)
                loss, _, _ = criterion(binary_logits, ternary_logits, labels)
                preds = hierarchical_predictions(binary_logits, ternary_logits)
            else:
                logits = model(flux, delta_t)
                loss = F.cross_entropy(logits, labels)
                preds = logits.argmax(dim=1)
            
            total_loss += loss.item()
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            if rank == 0:
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100.0 * correct / total:.2f}%'
                })
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100.0 * correct / total
    
    return avg_loss, accuracy


# =============================================================================
# MAIN TRAINING
# =============================================================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True, help='Path to data file (HDF5 or NPZ)')
    parser.add_argument('--output', type=str, default='../results', help='Output directory')
    parser.add_argument('--batch-size', type=int, default=128, help='Batch size per GPU')
    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs')
    parser.add_argument('--lr', type=float, default=0.0005, help='Learning rate')
    parser.add_argument('--weight-decay', type=float, default=0.0001, help='Weight decay')
    parser.add_argument('--warmup-epochs', type=int, default=3, help='Warmup epochs')
    parser.add_argument('--clip-norm', type=float, default=1.0, help='Gradient clipping')
    parser.add_argument('--d-model', type=int, default=256, help='Model dimension')
    parser.add_argument('--n-layers', type=int, default=4, help='Number of layers')
    parser.add_argument('--dropout', type=float, default=0.3, help='Dropout')
    parser.add_argument('--window-size', type=int, default=7, help='Conv window size')
    parser.add_argument('--hierarchical', action='store_true', help='Use hierarchical classification')
    parser.add_argument('--attention-pooling', action='store_true', help='Use attention pooling')
    parser.add_argument('--save-every', type=int, default=5, help='Save checkpoint every N epochs')
    parser.add_argument('--resume', type=str, default=None, help='Resume from checkpoint')
    parser.add_argument('--num-workers', type=int, default=0, help='DataLoader workers (use 0 for RAM loading)')
    parser.add_argument('--prefetch-factor', type=int, default=2, help='Prefetch factor')
    parser.add_argument('--accumulation-steps', type=int, default=1, help='Gradient accumulation')
    parser.add_argument('--use-prefetcher', action='store_true', help='Use prefetcher')
    
    args = parser.parse_args()
    
    # Setup distributed
    rank, local_rank, world_size = setup_distributed()
    device = torch.device(f'cuda:{local_rank}')
    
    # Create output directory
    output_dir = Path(args.output)
    checkpoint_dir = output_dir / 'checkpoints'
    if rank == 0:
        output_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    if rank == 0:
        print("\n" + "=" * 80)
        print("Roman Microlensing Classifier Training - ULTRA-FAST VERSION")
        print("=" * 80)
        print(f"Device: {device}")
        print(f"World size: {world_size}")
        print(f"Data file: {args.data}")
        print()
    
    # Load dataset with RAM loading
    if rank == 0:
        print("Loading datasets...")
    
    train_dataset = RAMLensingDataset(args.data, split='train', rank=rank)
    
    # Get normalization stats from train set
    norm_stats = train_dataset.get_norm_stats()
    
    val_dataset = RAMLensingDataset(args.data, split='val', rank=rank, norm_stats=norm_stats)
    
    if rank == 0:
        print()
    
    # Dataloaders
    if world_size > 1:
        train_sampler = DistributedSampler(train_dataset, shuffle=True)
        val_sampler = DistributedSampler(val_dataset, shuffle=False)
    else:
        train_sampler = None
        val_sampler = None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
        prefetch_factor=args.prefetch_factor if args.num_workers > 0 else None
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        sampler=val_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
        prefetch_factor=args.prefetch_factor if args.num_workers > 0 else None
    )
    
    if rank == 0:
        print(f"Dataloaders created:")
        print(f"  Batch size: {args.batch_size}")
        print(f"  Train batches: {len(train_loader)}")
        print(f"  Val batches: {len(val_loader)}")
        print(f"  Workers: {args.num_workers}")
        print()
    
    # Model
    model = RomanMicrolensingClassifier(
        d_model=args.d_model,
        n_layers=args.n_layers,
        dropout=args.dropout,
        window_size=args.window_size,
        hierarchical=args.hierarchical,
        use_attention_pooling=args.attention_pooling
    ).to(device)
    
    if rank == 0:
        n_params = sum(p.numel() for p in model.parameters())
        print(f"Model parameters: {n_params/1e6:.2f}M")
        print()
    
    # Wrap in DDP
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    
    # Criterion
    if args.hierarchical:
        criterion = HierarchicalLoss()
    else:
        criterion = nn.CrossEntropyLoss()
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
        fused=True
    )
    
    # Learning rate scheduler
    total_steps = len(train_loader) * args.epochs
    warmup_steps = len(train_loader) * args.warmup_epochs
    
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            progress = (step - warmup_steps) / (total_steps - warmup_steps)
            return 0.5 * (1 + np.cos(np.pi * progress))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # Resume if checkpoint provided
    start_epoch = 0
    best_val_acc = 0.0
    
    if args.resume and os.path.exists(args.resume):
        if rank == 0:
            print(f"Resuming from {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        if hasattr(model, 'module'):
            model.module.load_state_dict(checkpoint['model'])
        else:
            model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        scheduler.load_state_dict(checkpoint['scheduler'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_acc = checkpoint.get('best_val_acc', 0.0)
        if rank == 0:
            print(f"  Resumed from epoch {checkpoint['epoch']}")
            print(f"  Best val acc: {best_val_acc:.2f}%")
            print()
    
    # Training loop
    if rank == 0:
        print("=" * 80)
        print("Starting training...")
        print("=" * 80)
        print()
    
    for epoch in range(start_epoch, args.epochs):
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        # Train
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer,
            device, rank, epoch + 1
        )
        
        # Validate
        val_loss, val_acc = validate(
            model, val_loader, criterion, device, rank, epoch + 1
        )
        
        # Step scheduler
        scheduler.step()
        
        # Print epoch summary
        if rank == 0:
            print(f"\nEpoch {epoch + 1:3d} | "
                  f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            print()
        
        # Save checkpoint
        if rank == 0:
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
            
            checkpoint = {
                'epoch': epoch,
                'model': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'best_val_acc': best_val_acc,
                'args': vars(args)
            }
            
            # Save latest
            torch.save(checkpoint, checkpoint_dir / 'checkpoint_latest.pt')
            
            # Save best
            if is_best:
                torch.save(checkpoint, checkpoint_dir / 'checkpoint_best.pt')
                print(f"âœ“ New best model! Val acc: {val_acc:.2f}%")
            
            # Save periodic
            if (epoch + 1) % args.save_every == 0:
                torch.save(checkpoint, checkpoint_dir / f'checkpoint_epoch_{epoch+1:03d}.pt')
    
    if rank == 0:
        print("\n" + "=" * 80)
        print("Training complete!")
        print(f"Best validation accuracy: {best_val_acc:.2f}%")
        print("=" * 80)
    
    # Cleanup
    if world_size > 1:
        dist.destroy_process_group()


if __name__ == '__main__':
    main()
