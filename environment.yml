name: microlens
channels:
  - pytorch
  - conda-forge
  - defaults

dependencies:
  # Python
  - python=3.10

  # Core scientific computing
  - numpy=1.26.4
  - scipy=1.11.4
  - numba=0.59.0

  # Machine learning
  - scikit-learn=1.4.0

  # Visualization
  - matplotlib=3.8.2
  - seaborn=0.13.1

  # Data handling
  - h5py=3.10.0
  - pandas=2.2.0
  
  # Utilities
  - tqdm=4.66.1
  
  # Development
  - ipython=8.20.0
  - jupyter=1.0.0
  
  # Testing
  - pytest=8.0.0
  - pytest-cov=4.1.0
  
  # PyTorch - CUDA 12.1 (Recommended for NVIDIA A100/H100)
  # Change this section based on your hardware
  - pytorch::pytorch=2.2.0
  - pytorch::torchvision=0.17.0
  - pytorch::pytorch-cuda=12.1
  
  # Install via pip (not available through conda)
  - pip
  - pip:
      - joblib==1.3.2
      - VBBinaryLensing==3.6.0
      # Flash Attention - Optional but recommended (2-3x speedup)
      # Uncomment the line below if you have a compatible GPU (V100, A100, H100, RTX 3090/4090)
      # If installation fails, see README.md for alternative installation methods
      # - flash-attn==2.5.0 --no-build-isolation

# =============================================================================
# Flash Attention Installation (OPTIONAL - Recommended for Performance)
# =============================================================================
# Flash Attention provides 2-3x speedup for attention pooling operations.
# 
# Requirements:
#   - NVIDIA GPU with compute capability >= 7.0 (V100, A100, H100, RTX 3090/4090)
#   - CUDA 11.6+
#   - PyTorch 2.0+
#
# Installation (after creating conda environment):
#
#   # Standard installation
#   pip install flash-attn --no-build-isolation
#
#   # If that fails, try with ninja for faster compilation
#   pip install packaging ninja
#   pip install flash-attn --no-build-isolation
#
#   # On HPC clusters (e.g., bwForCluster)
#   module load devel/cuda/12.1
#   module load compiler/gnu/12.1
#   pip install flash-attn --no-build-isolation
#
# Verification:
#   python -c "from flash_attn import flash_attn_func; print('Flash Attention: OK')"
#
# NOTE: Flash Attention is OPTIONAL. The model automatically falls back to 
#       PyTorch's F.scaled_dot_product_attention if not available.
# =============================================================================

# =============================================================================
# Alternative PyTorch Installations (choose ONE, comment out others)
# =============================================================================
# The configuration above uses CUDA 12.1 which works on most modern NVIDIA GPUs
# including A100, H100, RTX 3090, RTX 4090, etc.
#
# If you have different hardware, replace the pytorch lines above with ONE of:
#
# --- CUDA 11.8 (for older NVIDIA GPUs) ---
# - pytorch::pytorch=2.2.0
# - pytorch::torchvision=0.17.0
# - pytorch::pytorch-cuda=11.8
#
# --- AMD ROCm 6.0 (for MI300, MI250 series) ---
# - pytorch::pytorch=2.2.0
# - pytorch::torchvision=0.17.0
# Then install via pip after creating environment:
# pip install torch==2.2.0 torchvision==0.17.0 --index-url https://download.pytorch.org/whl/rocm6.0
#
# --- CPU only (no GPU) ---
# - pytorch::pytorch=2.2.0
# - pytorch::torchvision=0.17.0
# - pytorch::cpuonly
#
# =============================================================================

# =============================================================================
# Installation Instructions
# =============================================================================
# 1. Create environment:
#    conda env create -f environment.yml
#
# 2. Activate environment:
#    conda activate microlens
#
# 3. Install Flash Attention (optional but recommended):
#    pip install flash-attn --no-build-isolation
#
# 4. Verify installation:
#    python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
#    python -c "import VBBinaryLensing; print('VBBinaryLensing: OK')"
#    python -c "import numba; print('Numba: OK')"
#    python -c "from flash_attn import flash_attn_func; print('Flash Attention: OK')"  # Optional
#
# 5. Update environment (if dependencies change):
#    conda env update -f environment.yml --prune
#
# 6. Export working environment (for reproducibility):
#    conda env export > environment_frozen.yml
# =============================================================================

# =============================================================================
# Tested Configurations
# =============================================================================
# Configuration 1: NVIDIA A100 (bwHPC cluster)
#   - CUDA 12.1
#   - PyTorch 2.2.0
#   - Python 3.10.13
#   - Flash Attention 2.5.0
#   - Works: ✓
#
# Configuration 2: NVIDIA H100 (bwHPC cluster)
#   - CUDA 12.1
#   - PyTorch 2.2.0
#   - Python 3.10.13
#   - Flash Attention 2.5.0
#   - Works: ✓
#
# Configuration 3: AMD MI300A (bwHPC cluster)
#   - ROCm 6.0
#   - PyTorch 2.2.0 (via pip)
#   - Python 3.10.13
#   - Flash Attention: Not supported (AMD)
#   - Works: ✓ (uses F.scaled_dot_product_attention fallback)
#
# Configuration 4: NVIDIA RTX 4090 (local workstation)
#   - CUDA 12.1
#   - PyTorch 2.2.0
#   - Python 3.10.13
#   - Flash Attention 2.5.0
#   - Works: ✓
#
# Configuration 5: Local CPU (development)
#   - CPU only
#   - PyTorch 2.2.0
#   - Python 3.10.13
#   - Works: ✓ (slow)
# =============================================================================

# =============================================================================
# Package Versions Rationale
# =============================================================================
# PyTorch 2.2.0: Stable release with torch.compile support
# NumPy 1.26.4: Compatible with Numba and PyTorch
# Numba 0.59.0: Latest stable with Python 3.10 support
# scikit-learn 1.4.0: Stable release for metrics
# h5py 3.10.0: Latest stable for HDF5 I/O
# VBBinaryLensing 3.6.0: Latest release for binary microlensing
# Flash Attention 2.5.0: Latest stable with PyTorch 2.2 support
# =============================================================================

# =============================================================================
# Troubleshooting
# =============================================================================
# 
# Flash Attention Installation Fails:
#   1. Ensure CUDA toolkit is installed: nvcc --version
#   2. Ensure compatible GPU: nvidia-smi (check compute capability)
#   3. Try: pip install packaging ninja && pip install flash-attn --no-build-isolation
#   4. On HPC: Load CUDA module first (module load devel/cuda/12.1)
#   5. If all else fails, the model works without flash attention (uses fallback)
#
# PyTorch CUDA Mismatch:
#   1. Check CUDA version: nvcc --version
#   2. Check PyTorch CUDA: python -c "import torch; print(torch.version.cuda)"
#   3. Reinstall PyTorch with matching CUDA version
#
# VBBinaryLensing Import Error:
#   1. Ensure pip installation: pip install VBBinaryLensing==3.6.0
#   2. Check for C++ compiler: gcc --version
#
# Numba JIT Compilation Slow:
#   1. First run compiles kernels (cached after)
#   2. Set NUMBA_CACHE_DIR to fast storage
# =============================================================================
