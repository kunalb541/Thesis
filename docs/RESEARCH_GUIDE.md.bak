# Research Guide - Systematic Benchmarking Methodology

Complete experimental workflow for thesis research.

---

## Research Questions

This work quantitatively addresses:

1. **Baseline Performance**: What classification accuracy is achievable across realistic binary parameter distributions?

2. **Cadence Dependence**: How does observation frequency affect detection performance?

3. **Early Detection**: How early can we reliably identify binary events with partial light curves?

4. **Physical Limits**: What are the fundamental detection limits imposed by binary topology (impact parameter u₀)?

5. **Photometric Quality**: How do measurement errors impact classification accuracy?

---

## Understanding Binary Microlensing

### Why Binary Detection Matters

Binary microlensing events reveal:
- **Exoplanets**: Low mass-ratio systems (q ~ 0.001 - 0.01)
- **Stellar Binaries**: Equal-mass systems (q ~ 0.3 - 1.0)
- **Black Hole Binaries**: Extreme mass ratios

Traditional PSPL fitting requires days of computation per event. With 20,000+ events/year from LSST/Roman, automated classification becomes essential.

### Key Physical Parameters

#### 1. Impact Parameter (u₀) - Most Critical

**Definition**: Minimum distance between source trajectory and lens (in Einstein radii)

**Physical Significance**:
- **Small u₀ (< 0.15)**: Close approach → High caustic crossing probability → Clearly distinguishable
- **Medium u₀ (0.15 - 0.30)**: Moderate distance → Sometimes distinguishable
- **Large u₀ (> 0.30)**: Far from lens → Fundamentally PSPL-like → Cannot reliably distinguish

**Research Implication**: ~15-25% of binary events have u₀ > 0.3 and are intrinsically indistinguishable. This is a **physical limit**, not an algorithm limitation.

#### 2. Separation (s)

**Definition**: Distance between binary components (in Einstein radii)

**Key Values**:
- **s ≈ 1.0**: Optimal caustic topology (wide, clear structures)
- **s < 0.7 or s > 2.0**: Smaller, fainter caustics (harder to detect)

#### 3. Mass Ratio (q)

**Definition**: m₂/m₁ (companion mass / primary mass)

**Regimes**:
- **Planetary**: q ~ 0.0001 - 0.01 (small planetary perturbations)
- **Brown Dwarf**: q ~ 0.01 - 0.1 (moderate perturbations)
- **Stellar**: q ~ 0.3 - 1.0 (symmetric caustics)

#### 4. Source Size (ρ)

**Definition**: Angular source radius / Einstein radius

**Effect**:
- **Small ρ (< 0.01)**: Sharp, spiky features (easier to detect)
- **Large ρ (> 0.05)**: Smoothed features (harder to detect)

#### 5. Einstein Timescale (tE)

Duration of microlensing event (days). Typical range: 10-200 days.

---

## Experimental Design

### Baseline Experiment

**Purpose**: Establish performance across mixed, realistic parameter ranges

**Configuration**:
```python
N_events = 1,000,000
N_pspl = 500,000
N_binary = 500,000

Binary Parameters:
  s: 0.1 - 2.5        # Wide range
  q: 0.1 - 1.0        # Planetary to stellar
  u0: 0.01 - 0.5      # Mix of easy/hard
  rho: 0.01 - 0.1     # Typical sources
  tE: 10 - 100 days   # Realistic timescales

Observational:
  Cadence: 20% missing observations (LSST-like)
  Photometry: 0.10 mag error (ground-based)
  Points: 1500 per light curve
```

**Commands**:
```bash
cd code

# Generate
python simulate.py \
    --n_pspl 500000 --n_binary 500000 \
    --output ../data/raw/baseline_1M.npz \
    --binary_params baseline \
    --seed 42

# Train
python train.py \
    --data ../data/raw/baseline_1M.npz \
    --experiment_name baseline \
    --epochs 50

# Evaluate
python evaluate.py \
    --experiment_name baseline \
    --data ../data/raw/baseline_1M.npz \
    --early_detection
```

**Expected Results**:
- Test Accuracy: 70-75%
- ROC AUC: 0.78-0.82
- Early detection (50% observed): 68-72%

---

### Systematic Experiments

After baseline, run these experiments:

#### 1. Cadence Experiments

**Purpose**: Quantify impact of observation frequency

| Experiment | Missing | Expected Acc | Survey Context |
|------------|---------|--------------|----------------|
| Dense      | 5%      | 75-80%       | Intensive follow-up |
| Baseline   | 20%     | 70-75%       | LSST nominal |
| Sparse     | 30%     | 65-70%       | Poor weather |
| Very Sparse| 40%     | 60-65%       | Limited coverage |

**Scientific Question**: Can we maintain >70% accuracy with sparse data?

**Commands**:
```bash
for cadence in 0.05 0.20 0.30 0.40; do
    name=$(echo $cadence | sed 's/0\.//')
    python simulate.py --n_pspl 100000 --n_binary 100000 \
        --output ../data/raw/cadence_${name}.npz \
        --cadence_mask_prob $cadence --seed 42
    python train.py --data ../data/raw/cadence_${name}.npz \
        --experiment_name cadence_${name} --epochs 50
    python evaluate.py --experiment_name cadence_${name} \
        --data ../data/raw/cadence_${name}.npz --early_detection
done
```

#### 2. Photometric Error Experiments

**Purpose**: Test robustness to measurement precision

| Experiment | Error (mag) | Quality | Expected Acc |
|------------|-------------|---------|--------------|
| Low        | 0.05        | Space-based (Roman) | 75-80% |
| Baseline   | 0.10        | Ground-based (LSST) | 70-75% |
| High       | 0.20        | Poor conditions | 65-70% |

**Scientific Question**: How much does photometric quality matter vs. cadence?

**Hypothesis**: Caustic features are sharp enough to survive moderate noise. Expect ~5-10% accuracy difference between 0.05 and 0.20 mag.

**Commands**:
```bash
for error in 0.05 0.10 0.20; do
    name=$(echo $error | sed 's/0\.//')
    python simulate.py --n_pspl 100000 --n_binary 100000 \
        --output ../data/raw/error_${name}.npz \
        --mag_error_std $error --seed 42
    python train.py --data ../data/raw/error_${name}.npz \
        --experiment_name error_${name} --epochs 50
    python evaluate.py --experiment_name error_${name} \
        --data ../data/raw/error_${name}.npz --early_detection
done
```

#### 3. Binary Topology Experiments

**Purpose**: Test performance across different caustic structures

| Experiment | Description | Parameters | Expected Acc |
|------------|-------------|------------|--------------|
| Distinct   | Clear caustics | s=0.8-1.5, q=0.1-0.5, u₀<0.15 | 80-90% |
| Planetary  | Planet-hosting | q=0.0001-0.01, varied u₀ | 70-80% |
| Stellar    | Equal-mass | q=0.3-1.0, varied u₀ | 60-75% |

**Scientific Question**: Can we identify the physical detection limit (u₀ threshold)?

**Key Analysis**: Plot accuracy vs. u₀ in bins. Expect performance drop at u₀ > 0.3.

**Commands**:
```bash
for topo in distinct planetary stellar; do
    python simulate.py --n_pspl 100000 --n_binary 100000 \
        --output ../data/raw/${topo}.npz \
        --binary_params ${topo} --seed 42
    python train.py --data ../data/raw/${topo}.npz \
        --experiment_name ${topo} --epochs 50
    python evaluate.py --experiment_name ${topo} \
        --data ../data/raw/${topo}.npz --early_detection
done
```

#### 4. Early Detection Analysis

**Purpose**: Test real-time classification capability

Evaluate model performance with 10%, 25%, 50%, 67%, 83%, 100% of observations.

**Scientific Question**: At what fraction can we trigger follow-up observations?

**Expected Results**:
```
10% observed:  50-55% accuracy (too early)
25% observed:  60-65% accuracy (marginal)
50% observed:  68-72% accuracy (acceptable for high-priority targets)
100% observed: 70-75% accuracy (baseline)
```

**Enabled automatically**: Use `--early_detection` flag in evaluate.py

---

## Analysis Workflow

### 1. Extract Results

```bash
# Generate summary table
python -c "
import json
from pathlib import Path

experiments = [
    'baseline', 'cadence_dense', 'cadence_sparse', 
    'error_low', 'error_high', 'distinct', 'planetary', 'stellar'
]

print(f'{'Experiment':<20} {'Test Acc':<12} {'ROC AUC':<10}')
print('-' * 45)

for exp in experiments:
    runs = sorted(Path('results').glob(f'{exp}_*'))
    if runs:
        summary = runs[-1] / 'summary.json'
        if summary.exists():
            data = json.load(open(summary))
            acc = data.get('final_test_acc', 0) * 100
            print(f'{exp:<20} {acc:>10.2f}%')
" > results_summary.txt

cat results_summary.txt
```

### 2. Generate Comparison Plots

**Cadence Analysis**:
```python
import matplotlib.pyplot as plt
import json
from pathlib import Path

cadences = [5, 20, 30, 40]
accuracies = []

for cad in cadences:
    exp = f'cadence_{cad:02d}'
    runs = sorted(Path('results').glob(f'{exp}_*'))
    if runs:
        with open(runs[-1] / 'summary.json') as f:
            data = json.load(f)
        accuracies.append(data['final_test_acc'] * 100)

plt.figure(figsize=(10, 6))
plt.plot(cadences, accuracies, 'o-', linewidth=2.5, markersize=10)
plt.xlabel('Missing Observations (%)', fontsize=12)
plt.ylabel('Test Accuracy (%)', fontsize=12)
plt.title('Performance vs. Observing Cadence', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)
plt.savefig('figures/cadence_comparison.png', dpi=300, bbox_inches='tight')
```

**Error Analysis**:
```python
errors = [0.05, 0.10, 0.20]
accuracies = []

for err in errors:
    exp = f'error_{int(err*100):02d}'
    runs = sorted(Path('results').glob(f'{exp}_*'))
    if runs:
        with open(runs[-1] / 'summary.json') as f:
            data = json.load(f)
        accuracies.append(data['final_test_acc'] * 100)

plt.figure(figsize=(10, 6))
plt.plot(errors, accuracies, 'o-', linewidth=2.5, markersize=10, color='red')
plt.xlabel('Photometric Error (mag)', fontsize=12)
plt.ylabel('Test Accuracy (%)', fontsize=12)
plt.title('Performance vs. Photometric Quality', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)
plt.savefig('figures/error_comparison.png', dpi=300, bbox_inches='tight')
```

### 3. Physical Interpretation

**u₀ Dependency Analysis**:

Extract binary event metadata and plot accuracy vs. u₀:

```python
import numpy as np

# Load baseline results with metadata
# Bin events by u0
u0_bins = np.linspace(0, 1, 11)
accuracies_by_u0 = []

for i in range(len(u0_bins)-1):
    u0_low, u0_high = u0_bins[i], u0_bins[i+1]
    # Filter events in this bin
    # Calculate accuracy for this subset
    # ...

plt.figure(figsize=(10, 6))
plt.plot(u0_bin_centers, accuracies_by_u0, 'o-', linewidth=2.5)
plt.axvline(x=0.3, color='red', linestyle='--', label='Physical limit (u₀ = 0.3)')
plt.xlabel('Impact Parameter u₀', fontsize=12)
plt.ylabel('Classification Accuracy (%)', fontsize=12)
plt.title('Accuracy vs. Impact Parameter', fontsize=14, fontweight='bold')
plt.legend()
plt.savefig('figures/u0_dependency.png', dpi=300, bbox_inches='tight')
```

**Expected Finding**: Clear performance drop at u₀ > 0.3, demonstrating fundamental physical limit.

---

## Thesis Structure

### Chapter 4: Results

#### 4.1 Baseline Performance

Report:
- Training/validation/test accuracy
- ROC curve and AUC
- Confusion matrix
- Sample predictions (6-12 examples)

**Key Message**: Demonstrate ~70-75% accuracy across realistic parameter distributions.

#### 4.2 Observational Dependence

**Cadence Study**:
- Plot: Accuracy vs. missing observations (5% to 40%)
- Finding: "Performance degrades gracefully with sparse data"
- Survey recommendation: "LSST nominal cadence (20% missing) sufficient for 70%+ accuracy"

**Photometric Quality Study**:
- Plot: Accuracy vs. photometric error (0.05 to 0.20 mag)
- Finding: "Caustic features robust to moderate noise"
- Survey recommendation: "Ground-based quality (0.10 mag) nearly as effective as space-based (0.05 mag)"

#### 4.3 Physical Limits

**Binary Topology Study**:
- Distinct (clear caustics): 80-90% accuracy
- Planetary: 70-80% accuracy
- Stellar: 60-75% accuracy

**u₀ Dependency**:
- Plot accuracy vs. impact parameter
- Demonstrate performance drop at u₀ > 0.3
- Conclusion: "~20% of binary events fundamentally indistinguishable due to large impact parameter"

#### 4.4 Early Detection Capability

- Plot: Accuracy vs. observation completeness
- Finding: "50% of observations sufficient for ~70% accuracy"
- Application: "Enable follow-up trigger decisions hours to days earlier than traditional fitting"

#### 4.5 Real-Time Performance

- Inference latency: <1 ms per event
- Throughput: 10,000+ events/second on single GPU
- Comparison: "~1000× faster than traditional PSPL fitting"

---

## Expected Contributions

Your thesis will provide:

1. **Quantitative Performance Estimates**: First comprehensive benchmarking of ML for binary microlensing

2. **Physical Interpretation**: Identification of u₀ threshold as fundamental detection limit

3. **Survey Operations Guidance**:
   - LSST: Nominal cadence sufficient
   - Roman: Space-based quality provides modest benefit
   - Follow-up: 50% completeness sufficient for triggering

4. **Open-Source Pipeline**: Ready for community adoption

5. **Real-Time Capability**: Demonstrate feasibility for alert stream processing

---

## Timeline

### Phase 1: Setup & Baseline (Weeks 1-2)
- Environment setup
- Generate baseline dataset (1M events)
- Train baseline model
- Full evaluation

### Phase 2: Systematic Experiments (Weeks 3-6)
- Cadence experiments (4 configs)
- Error experiments (3 configs)
- Topology experiments (4 configs)

### Phase 3: Analysis (Weeks 7-8)
- Generate all comparison plots
- u₀ dependency analysis
- Statistical significance tests
- Physical interpretation

### Phase 4: Writing (Weeks 9-12)
- Introduction & Methods
- Results chapter (figures & analysis)
- Discussion (physical interpretation)
- Conclusions & future work

---

## Key Metrics to Report

For each experiment:

1. **Classification Metrics**:
   - Accuracy, Precision, Recall, F1
   - ROC AUC
   - Confusion matrix

2. **Early Detection**:
   - Accuracy at 25%, 50%, 75% observation completeness

3. **Decision Time**:
   - Average timesteps to confident classification
   - Distribution of decision times

4. **Real-Time Performance**:
   - Inference latency (ms)
   - Throughput (events/sec)

---

## Statistical Significance

For comparing experiments:

```python
from scipy.stats import ttest_ind

# Load predictions from two experiments
acc1 = load_accuracies('baseline')
acc2 = load_accuracies('cadence_dense')

# T-test
t_stat, p_value = ttest_ind(acc1, acc2)

print(f"Cadence improvement: {(acc2.mean() - acc1.mean())*100:.2f}%")
print(f"p-value: {p_value:.4f}")
print(f"Significant: {'Yes' if p_value < 0.05 else 'No'}")
```

Report p-values for all comparisons in thesis.

---

## Survey Design Recommendations

Based on results, provide concrete guidance:

**For LSST**:
- "Nominal cadence (80% completeness) achieves 70-75% accuracy"
- "Dense follow-up (95% completeness) improves to 75-80%"
- "Recommendation: Standard survey cadence sufficient; intensive follow-up not required"

**For Roman**:
- "Space-based photometry (0.05 mag) vs. ground-based (0.10 mag): ~5% accuracy improvement"
- "Recommendation: Roman's photometric advantage modest for binary classification"

**For Follow-up**:
- "50% observation completeness: 68-72% accuracy"
- "Recommendation: Early trigger decisions possible after ~half event duration"

---

## Reproducibility

All results are fully reproducible:

1. **Fixed seeds**: Random seeds set in config.py
2. **Saved configurations**: All parameters logged
3. **Saved scalers**: Normalization preserved
4. **Exact versions**: requirements.txt pins all dependencies

To reproduce: Use commands exactly as specified above with same data paths.

---

You now have a complete experimental plan for your thesis! 🔬🔭

Follow this guide systematically, and you'll produce publication-quality results.