# Gravitational Microlensing Event Classification

**MSc Thesis Project** | University of Heidelberg | Prof. Dr. Joachim Wambsganß | Dr. Yiannis Tsapras

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![PyTorch 2.2+](https://img.shields.io/badge/pytorch-2.2+-orange.svg)](https://pytorch.org/)
[![Version 3.0.0](https://img.shields.io/badge/version-3.0.0-green.svg)](https://github.com/kunalb541/Thesis)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## Overview

Three-class classification of gravitational microlensing events using a CNN-GRU architecture:

- **Class 0**: Flat (no lensing)
- **Class 1**: PSPL (Point Source Point Lens)
- **Class 2**: Binary lens (planetary or stellar companion)

The model uses depthwise separable convolutions, flash attention pooling, **hierarchical classification with separate BCE losses**, and processes variable-length sequences with causal masking. Temporal encoding based on observation intervals (Δt) rather than absolute timestamps.

---

## Installation

### Quick Start

```bash
# Clone repository
git clone https://github.com/kunalb541/Thesis.git
cd Thesis

# Create conda environment (PyTorch CUDA 12.1 included)
conda env create -f environment.yml
conda activate microlens

# Install Flash Attention 
pip install flash-attn --no-build-isolation

# Verify installation
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import VBBinaryLensing; print('VBBinaryLensing: OK')"
python -c "from flash_attn import flash_attn_func; print('Flash Attention: OK')"
```

### Flash Attention Installation

**Note**: Flash Attention is optional. The model automatically falls back to PyTorch's `F.scaled_dot_product_attention` if Flash Attention is not available. Performance impact: ~2-3x slower attention pooling only (minimal overall impact).


Flash Attention for attention pooling. Installation requires:
- NVIDIA GPU with compute capability >= 7.0 (V100, A100, H100, RTX 3090/4090)
- CUDA 11.6+ 
- PyTorch 2.0+

**Standard Installation:**
```bash
pip install packaging ninja
pip install flash-attn --no-build-isolation
```

**If standard installation fails, try building from source:**
```bash
# Install build dependencies
pip install packaging ninja

# Clone and build
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
python setup.py install
```

**On HPC clusters (e.g., bwForCluster):**
```bash
# Load required modules first
module load devel/cuda/12.1
module load compiler/gnu/12.1

# Then install
pip install flash-attn --no-build-isolation
```

> **Note:** Flash Attention is optional. The model automatically falls back to PyTorch's `F.scaled_dot_product_attention` if Flash Attention is not available.

### GPU Configuration

**The environment.yml includes PyTorch CUDA 12.1 by default.**

For different hardware, edit `environment.yml` before creating the environment:

**AMD GPUs (ROCm 6.0)** - MI300 series:
```yaml
# In environment.yml, replace PyTorch lines with:
- pytorch::pytorch=2.2.0
- pytorch::torchvision=0.17.0
# Then after creating environment:
pip install torch==2.2.0 --index-url https://download.pytorch.org/whl/rocm6.0
```

**NVIDIA GPUs (CUDA 11.8)** - older cards:
```yaml
# In environment.yml, replace PyTorch lines with:
- pytorch::pytorch=2.2.0
- pytorch::torchvision=0.17.0
- pytorch::pytorch-cuda=11.8
```

**CPU only**:
```yaml
# In environment.yml, replace PyTorch lines with:
- pytorch::pytorch=2.2.0
- pytorch::torchvision=0.17.0
- pytorch::cpuonly
```

---

## Quick Start

Validate the complete pipeline:

```bash
cd code

# 1. Generate test dataset (300 events)
python simulate.py \
    --n_flat 100 \
    --n_pspl 100 \
    --n_binary 100 \
    --binary_preset baseline \
    --output ../data/raw/test.h5

# 2. Train model (5 epochs for validation)
python train.py \
    --data ../data/raw/test.h5 \
    --epochs 5 \
    --batch-size 32 \
    --hierarchical \
    --use-aux-head

# 3. Evaluate (experiment name is auto-generated by train.py)
# Full name (traditional - still works)
python evaluate.py --experiment-name d32_l2_hier --data test.h5

# The `--experiment-name` argument now supports partial matching - you don't need the exact directory name!
# List available experiments
ls ../results/checkpoints/

# Example output:
#   d32_l2_d32_l2_hier
#   d32_l2_hier_20241217_143022
#   d64_l4_baseline_20241218_093045

# Partial name 
python evaluate.py --experiment-name d32_l2_hier --data test.h5
python evaluate.py --experiment-name d64_l4 --data test.h5

# Very short partial (uses most recent match)
python evaluate.py --experiment-name d32 --data test.h5
```

---

## Data Generation

### Simulation Command 

```bash
python simulate.py \
    --n_flat 100000 \
    --n_pspl 100000 \
    --n_binary 100000 \
    --binary_preset baseline \
    --output ../data/raw/baseline.h5 \
    --num_workers 32 \
    --seed 42 \
    --oversample 1.3
```

**Binary Lens Presets:**

| Preset | Mass Ratio (q) | Separation (s) | Impact (u₀) | Caustic Required | Description |
|--------|----------------|----------------|-------------|:----------------:|-------------|
| `distinct` | 0.1 - 1.0 | 0.8 - 1.2 | 0.01 - 0.3 | Yes | Forced caustics for clear signals |
| `planetary` | 10⁻⁴ - 10⁻² | 0.6 - 1.6 | 0.01 - 0.3 | ✗ | Exoplanet detection regime |
| `stellar` | 0.1 - 1.0 | 0.3 - 3.0 | 0.01 - 0.5 | ✗ | Binary star systems |
| `baseline` | 10⁻⁴ - 1.0 | 0.3 - 3.0 | 0.01 - 0.5 | ✗ | Full parameter space |


**Output:** `<output>.h5` 
- Core datasets: `flux`, `delta_t`, `labels`, `timestamps`
- Parameters: `params_flat`, `params_pspl`, `params_binary` (structured arrays)
- Metadata: Mission duration, cadence, seed, version, etc.

**Data Format Note:**
- `flux` contains **magnifications** (A), not Jansky flux
- Current implementation applies uniform noise to magnification curves, independent of baseline source brightness. This simplification assumes a fixed characteristic magnitude. Future work should implement flux-dependent photon noise.
- A = 1.0 is baseline (no magnification)
- A > 1.0 means brighter (magnified)

---

## Training

### Single GPU 

```bash
python train.py \
    --data ../data/raw/baseline.h5 \
    --output ../results \
    --epochs 50 \
    --batch-size 64 \
    --lr 5e-4 \
    --weight-decay 1e-4 \
    --warmup-epochs 3 \
    --d-model 64 \
    --n-layers 4 \
    --dropout 0.3 \
    --window-size 5 \
    --hierarchical \
    --use-aux-head \
    --attention-pooling \
    --stage1-weight 1.0 \
    --stage2-weight 1.0 \
    --aux-weight 0.5
```

### Distributed Training (Multi-Node Multi-GPU)

**Using submit.sbatch (Recommended):**

```bash
# Submit the complete pipeline (data generation → training → evaluation)
sbatch submit.sbatch
```

The `submit.sbatch` script handles:
1. Data generation if files don't exist
2. Multi-node distributed training with torchrun
3. Automatic checkpoint resumption on timeout
4. Auto-submission of continuation jobs
5. Automatic evaluation job submission after training completes

**Manual SLURM Execution:**

```bash
# Allocate resources
salloc --partition=gpu_a100_short --nodes=12 --gres=gpu:4 --exclusive --time=00:30:00

cd ~/Thesis/code
conda activate microlens

# Environment setup
export PYTHONWARNINGS="ignore"
export TORCH_DISTRIBUTED_DEBUG=OFF
export NCCL_DEBUG=WARN
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_NODELIST" | head -n 1)
export MASTER_PORT=29500

# Launch distributed training
srun torchrun \
    --nnodes=$SLURM_NNODES \
    --nproc-per-node=4 \
    --rdzv-backend=c10d \
    --rdzv-endpoint="${MASTER_ADDR}:${MASTER_PORT}" \
    --rdzv-id="train-$(date +%s)" \
    train.py \
    --data /tmp/train.h5 \
    --output ../results \
    --epochs 300 \
    --batch-size 64 \
    --lr 5e-4 \
    --hierarchical \
    --use-aux-head \
    --attention-pooling \
    --save-every 2
```

### Hierarchical Classification 

The hierarchical mode uses a two-stage classification approach:

```
Stage 1: Flat vs Non-Flat (Binary Cross-Entropy)
    ↓
Stage 2: PSPL vs Binary (Binary Cross-Entropy, only for Non-Flat)
    ↓
Auxiliary: 3-class Cross-Entropy (gradient stability)
```

**Loss Function:**
```
L_total = λ₁ × L_stage1 + λ₂ × L_stage2 + λ_aux × L_auxiliary
```

**Key Arguments:**
- `--hierarchical`: Enable hierarchical classification
- `--use-aux-head`: Add auxiliary 3-class head for gradient stability
- `--stage1-weight`: Weight for Stage 1 loss (default: 1.0)
- `--stage2-weight`: Weight for Stage 2 loss (default: 1.0)
- `--aux-weight`: Weight for auxiliary loss (default: 0.5)
- `--stage2-temperature`: Temperature scaling for Stage 2 (default: 1.0)

### Checkpoint Resumption

Training automatically resumes from checkpoints:

```bash
# Resume from latest checkpoint
python train.py \
    --data baseline.h5 \
    --epochs 300 \
    --resume ../results/checkpoints/checkpoint_latest.pt

# Resume from specific epoch
python train.py \
    --data baseline.h5 \
    --epochs 300 \
    --resume ../results/experiment_20241216/checkpoint_epoch_50.pt
```

**Training Outputs** (saved in `results/<timestamp>/`):

| File | Description |
|------|-------------|
| `best_model.pt` | Best checkpoint (highest validation accuracy) |
| `checkpoint_epoch_N.pt` | Periodic checkpoints |
| `checkpoint_latest.pt` | Most recent checkpoint (for resumption) |
| `training.log` | Complete training log with metrics |
| `config.json` | Full configuration and hyperparameters |

---

## Evaluation

### Standard Evaluation

```bash
python evaluate.py \
    --experiment-name <experiment_name> \
    --data ../data/test/test.h5 \
    --batch-size 512 \
    --early-detection \
    --n-evolution-per-type 5 \
    --colorblind-safe
```

**Important:** Train.py auto-generates names like `baseline_20241216_143022`. You can find the name by:

```bash
ls ../results/
```

### Advanced Options

```bash
python evaluate.py \
    --experiment-name d32_l2_hier  # Partial match \
    --data test.h5 \
    --save-formats png pdf svg \
    --colorblind-safe \
    --n-samples 100000

# Verbose logging for debugging
python evaluate.py \
    --experiment-name d32_l2_hier  # Partial match \
    --data test.h5 \
    --verbose
```

**Evaluation Outputs** (saved in `results/<experiment>/eval_<dataset>_<timestamp>/`):

| File | Description |
|------|-------------|
| `evaluation_summary.json` | Overall metrics and configuration |
| `confusion_matrix.png` | Normalized confusion matrix heatmap |
| `roc_curves.png` | One-vs-rest ROC curves with AUC scores |
| `calibration.png` | Reliability diagram with confidence histograms |
| `u0_dependency.png` | Binary accuracy vs. impact parameter |
| `temporal_bias_check.png` | t₀ distribution comparison (KS-test) |
| `early_detection_curve.png` | Accuracy vs. observation completeness |
| `evolution_<class>_<idx>.png` | Probability evolution (3-panel) |
| `example_light_curves.png` | Grid of example classifications |
| `per_class_metrics.png` | Precision/recall/F1 bar chart |
| `predictions.npz` | Raw predictions and probabilities |
| `classification_report.txt` | Detailed per-class metrics |

**Metrics Computed:**
- Overall: accuracy, precision, recall, F1-score 
- Per-class: precision, recall, F1 for Flat, PSPL, Binary
- AUROC: macro and weighted averages
- Calibration reliability
- Bootstrap confidence intervals
- Early detection performance at [10%, 20%, 30%, 50%, 70%, 100%] completeness
- Impact parameter (u₀) dependency for binary events
- Temporal bias check (Kolmogorov-Smirnov test)

---

## Complete Pipeline (submit.sbatch)

The `submit.sbatch` script provides end-to-end automation:

```bash
sbatch submit.sbatch
```

**Pipeline Flow:**

```
1. Check if training is already complete
   ├─ If complete → Submit evaluation job → Exit
   └─ If incomplete → Continue to step 2
                      
2. Submit continuation job (dependency: afterany)

3. Generate data if not exists
   ├─ Training data: simulate.py → train.h5
   └─ Test data: simulate.py → test.h5

4. Copy data to /tmp on all nodes

5. Launch distributed training (torchrun)
   └─ With checkpoint resumption if available

6. On completion:
   ├─ Cancel continuation job
   └─ Submit evaluation job 
```

**Configuration (edit in submit.sbatch):**

```bash
# Training hyperparameters
BATCH_SIZE=64
MAX_EPOCHS=300
LEARNING_RATE=0.0005

# Model architecture
D_MODEL=64
N_LAYERS=4
DROPOUT=0.3

# Hierarchical loss weights
STAGE1_WEIGHT=1.0
STAGE2_WEIGHT=1.0
AUX_WEIGHT=0.5

# Evaluation settings
EVAL_BATCH_SIZE=512
```

---

## Architecture

### Model: RomanMicrolensingClassifier 

```
Input: Flux [B, N], Time Intervals Δt [B, N]
  │
  ├─ Input Projection: Linear(2 → d_model)
  │
Feature Extraction: Depthwise Separable Conv (2 blocks, causal)
  │  ├─ Block 1: kernel=5, dilation=1
  │  └─ Block 2: kernel=5, dilation=2 (multi-scale)
  │
Recurrent Core: Stacked GRU (CuDNN-fused)
  │  └─ n_layers with dropout between layers
  │
Layer Normalization + Residual Connection
  │
Temporal Pooling:
  │  ├─ Attention Pooling (multi-head, learnable query)
  │  └─ OR Mean Pooling (masked by sequence length)
  │
┌─────────────────────────────────────────────────────┐
│ Hierarchical Classification (v3.0.0)                │
│                                                     │
│  Shared Trunk: Linear + LayerNorm + SiLU + Dropout  │
│        │                                            │
│        ├─ Stage 1 Head: Flat vs Non-Flat (BCE)      │
│        ├─ Stage 2 Head: PSPL vs Binary (BCE)        │
│        └─ Auxiliary Head: 3-class (CE)              │
└─────────────────────────────────────────────────────┘
  │
Output: Log-Probabilities [B, 3]
```

**Default Configuration:**

| Parameter | Value | Description |
|-----------|-------|-------------|
| `d_model` | 64 | Hidden dimension |
| `n_layers` | 4 | GRU layers |
| `dropout` | 0.3 | Dropout rate |
| `window_size` | 5 | Conv kernel size |
| `max_seq_len` | 2400 | Maximum sequence length |
| `n_classes` | 3 | Output classes |
| `hierarchical` | True | Two-stage classification |
| `attention_pooling` | True | Multi-head attention pooling |
| `head_init_std` | 0.15 | Classification head initialization |

**Parameter Count:** ~100-500K (varies with d_model and n_layers)

---

## Project Structure

```
Thesis/
├── code/
│   ├── simulate.py
│   ├── train.py
│   ├── model.py
│   ├── evaluate.py
│   ├── submit.sbatch
│   └── logs/            
│
├── data/
│   ├── raw/
│   └── test/
│
├── results/
│   └── checkpoints/
│ 
├── .gitignore
├── environment.yml
├── README.md
└── LICENSE
```

---

## Version History

### v3.0.1 (Current) - December 2024

**Cosmetic Fixes in evaluate.py:**
- Fixed confusion matrix text sizing and overlap
- ROC curves legend moved outside plot area
- u0 dependency plot count annotations repositioned
- Temporal bias check legend/text overlap resolved
- Calibration curve legend positioning improved
- Per-class metrics bar chart label overlap prevented
- Evolution plots panel spacing increased
- Example light curves grid spacing improved


### v3.0.0 (Current) - December 2024

**All Components Synchronized:**
- `train.py` v3.0.0
- `model.py` v3.0.0
- `simulate.py` v3.0.0
- `evaluate.py` v3.0.0
- `submit.sbatch` v3.0.0

**Key Changes:**
- All magic numbers replaced with named `Final` constants
- CLI argument validation in simulate.py
- Safe multiprocessing (set_start_method in main guard)
- MemoryError propagation for critical failures
- HDF5 backward compatibility ('flux' and 'mag' keys)
- Automatic evaluation job submission after training
- Enhanced documentation and docstrings

### v2.7.0 - v2.9.0

- Added `get_valid_lengths()` function (was causing NameError)
- Added `ROMAN_ZP_FLUX_JY` constant
- Fixed `plot_evolution_for_class` indentation
- Hierarchical classification with separate BCE losses
- Auxiliary 3-class head for gradient stability
- Per-class recall logging during training

### v2.6.0

- Critical fix: Hierarchical mode probability computation
- Uses `torch.exp()` instead of `F.softmax()` for log-probabilities

---

## Physical Parameter Ranges

**Observational Configuration:**

| Parameter | Value | Description |
|-----------|-------|-------------|
| Temporal sampling | ~12.1 min | Roman-like cadence |
| Missing observations | 5% | Uniform random gaps |
| Photometric error | Realistic | Roman F146 detector model |
| Mission duration | 200 days | POC (full Roman: 5 years) |
| Max sequence length | 2400 | Observations per event |
| Source magnitude | 18-24 AB | Baseline brightness |

**Microlensing Parameters:**

| Parameter | Range | Description |
|-----------|-------|-------------|
| Einstein timescale (t_E) | 5-30 days | Event duration |
| Peak time (t₀) | 20-180 days | 10%-90% of mission |
| Impact parameter (u₀) | 0.001-1.0 | Closest approach |
| Binary separation (s) | 0.1-3.0 | Einstein radii |
| Mass ratio (q) | 10⁻⁴ - 1.0 | Secondary/primary mass |
| Source radius (ρ) | 10⁻³ - 0.1 | Einstein radii |

---

## Citation

```bibtex
@mastersthesis{bhatia2025microlensing,
  title={From Light Curves to Labels: Machine Learning in Microlensing Event Classification},
  author={Bhatia, Kunal},
  year={2025},
  school={University of Heidelberg},
  type={MSc Thesis},
  note={Advisor: Prof. Dr. Joachim Wambsganß}
}
```

---

## License

MIT License - See [LICENSE](LICENSE) for details.

---

## References

**Key Publications:**

1. Bozza, V. (2010). "VBBinaryLensing: A C++ library for microlensing." MNRAS, 408, 2188-2196.
2. Zhu, W., et al. (2017). "Mass Measurements from Space-based Microlensing." ApJ, 849, L31.
3. Johnson, S. A., et al. (2020). "Nancy Grace Roman Space Telescope Predictions." AJ, 160, 123.
4. Gaudi, B. S. (2012). "Microlensing Surveys for Exoplanets." ARA&A, 50, 411-453.

**Survey Resources:**
- OGLE: http://ogle.astrouw.edu.pl/
- MOA: https://www.massey.ac.nz/~iabond/moa/
- Nancy Grace Roman: https://roman.gsfc.nasa.gov/

**Machine Learning in Microlensing:**
- Lam et al. (2020): ML classification of OGLE events
- Godines et al. (2019): CNN-based microlensing detection

---

## Acknowledgments

- Prof. Dr. Joachim Wambsganß
- Dr. Yiannis Tsapras
- bwForCluster HPC resources

---
